{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_vggface\n",
    "from keras_vggface.vggface import VGGFace\n",
    "# Had to change code in keras vggface to make it work changed model.py line 20 as shown here https://stackoverflow.com/a/71379206\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras_vggface.utils\n",
    "import PIL\n",
    "import os\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 files belonging to 4 classes.\n",
      "Using 240 files for training.\n",
      "Found 300 files belonging to 4 classes.\n",
      "Using 60 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_seed = 42\n",
    "split = 0.2\n",
    "\n",
    "train_dataset = keras.utils.image_dataset_from_directory('./Faces', shuffle=True, image_size=(224, 224), batch_size=8, seed=train_seed, validation_split=split, subset='training')\n",
    "val_dataset = keras.utils.image_dataset_from_directory('./Faces', shuffle=True, image_size=(224, 224), batch_size=8, seed=train_seed, validation_split=split, subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.2), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_base = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras_vggface.vggface import VGGFace\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "resnet_base.trainable = False\n",
    "last_layer = resnet_base.get_layer('avg_pool').output\n",
    "\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "x = data_augmentation(inputs)\n",
    "x = resnet_base(x)\n",
    "x = Flatten(name='flatten')(x)\n",
    "\n",
    "out = Dense(num_classes, name='classifier')(x)\n",
    "\n",
    "custom_resnet_model = keras.Model(inputs, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "\n",
    "custom_resnet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 22:31:05.739322: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - ETA: 0s - loss: 1.5815 - accuracy: 0.4292"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 22:31:09.235602: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 7s 131ms/step - loss: 1.5815 - accuracy: 0.4292 - val_loss: 0.4504 - val_accuracy: 0.8167\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 2s 81ms/step - loss: 0.9905 - accuracy: 0.6208 - val_loss: 0.2468 - val_accuracy: 0.9833\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 2s 81ms/step - loss: 0.6354 - accuracy: 0.7792 - val_loss: 0.1926 - val_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 2s 81ms/step - loss: 0.7563 - accuracy: 0.7583 - val_loss: 0.1679 - val_accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 0.6352 - accuracy: 0.7750 - val_loss: 0.1555 - val_accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 0.5206 - accuracy: 0.8250 - val_loss: 0.1432 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 0.4936 - accuracy: 0.8333 - val_loss: 0.1332 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 0.4606 - accuracy: 0.8333 - val_loss: 0.1228 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 0.4140 - accuracy: 0.8542 - val_loss: 0.1189 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 2s 81ms/step - loss: 0.3724 - accuracy: 0.8750 - val_loss: 0.1096 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 0.3811 - accuracy: 0.8792 - val_loss: 0.1067 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 0.3370 - accuracy: 0.8875 - val_loss: 0.1012 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 0.3120 - accuracy: 0.9000 - val_loss: 0.0938 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 0.3390 - accuracy: 0.8625 - val_loss: 0.0947 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 2s 81ms/step - loss: 0.2982 - accuracy: 0.9125 - val_loss: 0.0896 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 0.2912 - accuracy: 0.9167 - val_loss: 0.0879 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 0.2620 - accuracy: 0.9333 - val_loss: 0.0863 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 2s 80ms/step - loss: 0.2511 - accuracy: 0.9458 - val_loss: 0.0810 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 2s 78ms/step - loss: 0.2694 - accuracy: 0.9125 - val_loss: 0.0772 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 2s 79ms/step - loss: 0.2688 - accuracy: 0.9125 - val_loss: 0.0766 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = custom_resnet_model.fit(train_dataset, epochs=20, validation_data=val_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
    "import tensorflow_privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 files belonging to 4 classes.\n",
      "Using 240 files for training.\n",
      "Found 300 files belonging to 4 classes.\n",
      "Using 60 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = keras.utils.image_dataset_from_directory('./Faces', shuffle=True, image_size=(224, 224), batch_size=16, seed=train_seed, validation_split=split, subset='training')\n",
    "val_dataset = keras.utils.image_dataset_from_directory('./Faces', shuffle=True, image_size=(224, 224), batch_size=16, seed=train_seed, validation_split=split, subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=tf.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_multiplier = 1.1\n",
    "l2_norm_clip = 5\n",
    "num_microbatches = 16\n",
    "learning_rate = 0.0001\n",
    "\n",
    "optimizer = tensorflow_privacy.DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "\n",
    "custom_resnet_model.compile(optimizer=optimizer,\n",
    "    loss = loss,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 22:31:58.096225: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - ETA: 0s - loss: 6.1115 - accuracy: 0.3667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 22:32:02.200694: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 7s 309ms/step - loss: 6.1115 - accuracy: 0.3667 - val_loss: 9.9964 - val_accuracy: 0.1333\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 3s 206ms/step - loss: 7.8521 - accuracy: 0.2042 - val_loss: 10.3319 - val_accuracy: 0.1333\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 3s 187ms/step - loss: 8.0231 - accuracy: 0.2500 - val_loss: 10.1482 - val_accuracy: 0.1333\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 3s 183ms/step - loss: 8.6566 - accuracy: 0.2625 - val_loss: 10.2234 - val_accuracy: 0.1167\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 3s 181ms/step - loss: 8.4014 - accuracy: 0.2125 - val_loss: 10.6941 - val_accuracy: 0.1167\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 3s 181ms/step - loss: 7.8116 - accuracy: 0.2583 - val_loss: 10.7594 - val_accuracy: 0.1167\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 3s 181ms/step - loss: 8.5918 - accuracy: 0.2708 - val_loss: 10.6923 - val_accuracy: 0.1167\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 3s 180ms/step - loss: 8.9458 - accuracy: 0.1958 - val_loss: 10.6998 - val_accuracy: 0.1167\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 3s 183ms/step - loss: 7.9098 - accuracy: 0.2375 - val_loss: 10.5628 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 3s 183ms/step - loss: 8.7504 - accuracy: 0.2500 - val_loss: 10.6994 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 3s 182ms/step - loss: 8.4976 - accuracy: 0.2500 - val_loss: 10.6421 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 3s 180ms/step - loss: 8.0581 - accuracy: 0.2042 - val_loss: 11.0407 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 3s 185ms/step - loss: 7.7785 - accuracy: 0.2708 - val_loss: 10.8395 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 3s 186ms/step - loss: 7.9592 - accuracy: 0.2375 - val_loss: 10.9700 - val_accuracy: 0.1000\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 3s 188ms/step - loss: 8.7420 - accuracy: 0.1708 - val_loss: 10.8370 - val_accuracy: 0.1000\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 3s 194ms/step - loss: 8.4596 - accuracy: 0.1917 - val_loss: 11.0412 - val_accuracy: 0.1000\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 3s 183ms/step - loss: 7.7363 - accuracy: 0.2292 - val_loss: 10.8372 - val_accuracy: 0.1000\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 3s 182ms/step - loss: 8.2956 - accuracy: 0.2500 - val_loss: 10.9210 - val_accuracy: 0.1000\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 3s 180ms/step - loss: 8.1135 - accuracy: 0.1875 - val_loss: 11.2535 - val_accuracy: 0.1000\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 3s 181ms/step - loss: 8.4746 - accuracy: 0.2167 - val_loss: 11.3225 - val_accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "history = custom_resnet_model.fit(train_dataset, epochs=20, validation_data=val_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
